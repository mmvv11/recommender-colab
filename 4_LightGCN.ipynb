{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmvv11/recommender-colab/blob/main/4_LightGCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "119ab203",
      "metadata": {
        "id": "119ab203"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from scipy import sparse as ssp\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4331e385",
      "metadata": {
        "id": "4331e385"
      },
      "source": [
        "하이퍼파라미터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27ed21e0",
      "metadata": {
        "id": "27ed21e0"
      },
      "outputs": [],
      "source": [
        "device=\"cuda\" # 디바이스\n",
        "n_neg=4 # 네거티브 샘플링 갯수\n",
        "n_layers = 2 # GNN 레이어 갯수\n",
        "dropout=0.0 # dropout rate\n",
        "data_path = \"./ml-100k_splited.pkl\" # 데이터셋 경로\n",
        "batch_size = 1024 # 훈련 데이터 배치 사이즈\n",
        "emb_size = 8 # 임베딩 크기\n",
        "lr = 1e-3\n",
        "top_k = 20\n",
        "n_epoch=10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f22c43b",
      "metadata": {
        "id": "4f22c43b"
      },
      "source": [
        "## 데이터 로딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ccd4f9",
      "metadata": {
        "id": "03ccd4f9"
      },
      "outputs": [],
      "source": [
        "with open(data_path, \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "train, val, test, all_items, user2id, id2user, item2id, id2item = data.values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db750558",
      "metadata": {
        "id": "db750558"
      },
      "outputs": [],
      "source": [
        "class MLDataset(Dataset):\n",
        "    def __init__(self, df, all_items, n_neg=4):\n",
        "        super().__init__()\n",
        "        self.n_neg=n_neg\n",
        "        self.users, self.items, self.labels = self.get_data(df, all_items)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.items[idx], self.labels[idx]\n",
        "\n",
        "    def get_data(self, df, all_items):\n",
        "        users, pos_items, neg_items = [], [], []\n",
        "        user_item_set = set(zip(df['user'], df['item']))\n",
        "        for u, i in user_item_set:\n",
        "            users.append(u)\n",
        "            pos_items.append(i)\n",
        "            for _ in range(self.n_neg):\n",
        "                neg_item = np.random.choice(all_items)\n",
        "                while (u, neg_item) in user_item_set:\n",
        "                    neg_item = np.random.choice(all_items)\n",
        "                neg_items.append(neg_item)\n",
        "        return torch.tensor(users).to(device), torch.tensor(pos_items).to(device), torch.tensor(neg_items).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5975a7a",
      "metadata": {
        "id": "c5975a7a"
      },
      "outputs": [],
      "source": [
        "train_dataset = MLDataset(train, all_items, )\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65491bb2",
      "metadata": {
        "id": "65491bb2"
      },
      "outputs": [],
      "source": [
        "user_consumed = train.groupby(\"user\")['item'].apply(list).to_dict()\n",
        "val_true = val.groupby(\"user\")['item'].apply(list).to_dict()\n",
        "test_true = test.groupby(\"user\")['item'].apply(list).to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f4f89d1",
      "metadata": {
        "id": "6f4f89d1"
      },
      "source": [
        "# 모델링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8980a96",
      "metadata": {
        "id": "f8980a96"
      },
      "outputs": [],
      "source": [
        "class LightGCN(nn.Module):\n",
        "    def __init__(self, n_users, n_items, emb_size, n_layers, user_consumed):\n",
        "        super().__init__()\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.emb_user = nn.Embedding(n_users, emb_size)\n",
        "        self.emb_item = nn.Embedding(n_items, emb_size)\n",
        "        self.n_layers = n_layers\n",
        "        self.user_consumed = user_consumed\n",
        "        self.laplacian_matrix = self._build_laplacian_matrix()\n",
        "        self._init_weight()\n",
        "\n",
        "    def _init_weight(self):\n",
        "        nn.init.normal_(self.emb_user.weight, std=1e-2)\n",
        "        nn.init.normal_(self.emb_item.weight, std=1e-2)\n",
        "\n",
        "    def _build_laplacian_matrix(self):\n",
        "        R = ssp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
        "        for u in range(self.n_users):\n",
        "            items = self.user_consumed[u]\n",
        "            R[u, items] = 1.0\n",
        "        R = R.tolil()\n",
        "\n",
        "        adj_matrix = ssp.dok_matrix(\n",
        "            (self.n_users+self.n_items, self.n_items+self.n_users), dtype=np.float32\n",
        "        )\n",
        "        adj_matrix[:self.n_users, self.n_users:]=R\n",
        "        adj_matrix[self.n_users:, :self.n_users]=R.T\n",
        "        # adj_matrix = adj_matrix.tocsr()\n",
        "\n",
        "        row_sum = np.array(adj_matrix.sum(axis=1)) # adj의 row sum은 각 유저/아이템 노드의 차수를 의미\n",
        "        diag_inv = np.power(row_sum, -0.5).flatten()\n",
        "        diag_inv[np.isinf(diag_inv)] = 0.0\n",
        "        diag_matrix_inv = ssp.diags(diag_inv) # D^(-1/2)\n",
        "\n",
        "        coo = diag_matrix_inv.dot(adj_matrix).dot(diag_matrix_inv).tocoo() # D^(-1/2) * A * D^(-1/2)\n",
        "        indices = torch.from_numpy(np.array([coo.row, coo.col]))\n",
        "        values = torch.from_numpy(coo.data)\n",
        "        laplacian_matrix = torch.sparse_coo_tensor(\n",
        "            indices, values, coo.shape, dtype=torch.float32, device=device\n",
        "        )\n",
        "        return laplacian_matrix\n",
        "\n",
        "    def emb_propagation(self):\n",
        "        all_emb = [\n",
        "            torch.cat(\n",
        "                [self.emb_user.weight, self.emb_item.weight], dim=0\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        for _ in range(n_layers):\n",
        "            layered_emb = torch.sparse.mm(self.laplacian_matrix, all_emb[-1])\n",
        "            all_emb.append(layered_emb)\n",
        "\n",
        "        all_emb = torch.stack(all_emb, dim=1)\n",
        "        mean_emb = torch.mean(all_emb, dim=1)\n",
        "\n",
        "        layered_emb_user, layered_emb_item = torch.split(mean_emb, [self.n_users, self.n_items])\n",
        "        return layered_emb_user, layered_emb_item\n",
        "\n",
        "    def forward(self, users, pos_items, neg_items=None):\n",
        "         # 임베딩 값을 가져오고 (복수, 단수형으로 변수명 구분할 것.)\n",
        "        layered_emb_users, layered_emb_items = self.emb_propagation()\n",
        "        # propagation 후 user, pos, neg\n",
        "        layered_emb_user, layered_pos_emb, layered_neg_emb = layered_emb_users[users], layered_emb_items[pos_items], layered_emb_items[neg_items] if neg_items != None else None\n",
        "\n",
        "        return layered_emb_user, layered_pos_emb, layered_neg_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "442f12b2",
      "metadata": {
        "id": "442f12b2"
      },
      "source": [
        "모델, 손실 함수, 옵티마이저 정의"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def bpr_loss(layered_emb_user, layered_pos_emb, layered_neg_emb):\n",
        "    # reg loss는 forward return에 있는 init emb를 활용해서 별도 처리하기.\n",
        "    pos_score = (layered_emb_user * layered_pos_emb).sum(dim=1)\n",
        "    neg_score = (layered_emb_user * layered_neg_emb).sum(dim=1)\n",
        "    log_sigmoid = F.logsigmoid(pos_score - neg_score)\n",
        "    return torch.negative(torch.mean(log_sigmoid))"
      ],
      "metadata": {
        "id": "pKvXe5Jq5cPl"
      },
      "id": "pKvXe5Jq5cPl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cYQB3mErDAJc"
      },
      "id": "cYQB3mErDAJc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1935f00a",
      "metadata": {
        "id": "1935f00a"
      },
      "outputs": [],
      "source": [
        "n_users, n_items = len(user2id), len(item2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dc392a1",
      "metadata": {
        "id": "8dc392a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b00487-2129-45d6-a206-aecfad250697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-1284aad0aad4>:33: RuntimeWarning: divide by zero encountered in power\n",
            "  diag_inv = np.power(row_sum, -0.5).flatten()\n"
          ]
        }
      ],
      "source": [
        "model = LightGCN(n_users, n_items, emb_size, n_layers, user_consumed)\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e024b9",
      "metadata": {
        "id": "41e024b9"
      },
      "source": [
        "메트릭\n",
        "* precision\n",
        "* recall\n",
        "* nDCG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1b61a75",
      "metadata": {
        "id": "e1b61a75"
      },
      "outputs": [],
      "source": [
        "def get_precision(pred, true, k=20):\n",
        "    intersection = set(pred).intersection(set(true))\n",
        "    return len(intersection)/ k\n",
        "\n",
        "def get_recall(pred, true, k=20):\n",
        "    intersection = set(pred).intersection(set(true))\n",
        "    return len(intersection)/len(true)\n",
        "\n",
        "def get_nDCG(pred, true, k=20):\n",
        "    intersection, _, idx_in_pred = np.intersect1d(true, pred, assume_unique=True, return_indices=True)\n",
        "    if intersection.size == 0:\n",
        "        return 0\n",
        "    rank_list = np.zeros(k, np.float32)\n",
        "    rank_list[idx_in_pred] = 1\n",
        "    ideal_list = np.sort(rank_list)[::-1]\n",
        "    dcg = np.sum(rank_list/np.log2(np.arange(2, k+2)))\n",
        "    idcg = np.sum(ideal_list /np.log2(np.arange(2, k+2)))\n",
        "    return dcg/idcg"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35389cb5",
      "metadata": {
        "id": "35389cb5"
      },
      "source": [
        "train process 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c97e5dc7",
      "metadata": {
        "id": "c97e5dc7",
        "outputId": "211338ba-bc63-4edd-fddb-73315e12246b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 79/79 [00:02<00:00, 27.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, total_loss: 0.6882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "eval: 100%|██████████| 943/943 [00:02<00:00, 333.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall:0.1896\n",
            "precision:0.0747\n",
            "ndcg:0.3455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 79/79 [00:01<00:00, 58.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 2, total_loss: 0.6439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "eval: 100%|██████████| 943/943 [00:03<00:00, 288.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall:0.1894\n",
            "precision:0.0742\n",
            "ndcg:0.3440\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 79/79 [00:01<00:00, 59.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 3, total_loss: 0.5627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "eval: 100%|██████████| 943/943 [00:02<00:00, 358.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall:0.1896\n",
            "precision:0.0744\n",
            "ndcg:0.3439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 79/79 [00:01<00:00, 59.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 4, total_loss: 0.4838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "eval: 100%|██████████| 943/943 [00:02<00:00, 368.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall:0.1884\n",
            "precision:0.0745\n",
            "ndcg:0.3451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 79/79 [00:01<00:00, 59.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 5, total_loss: 0.4301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "eval: 100%|██████████| 943/943 [00:03<00:00, 282.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall:0.1886\n",
            "precision:0.0751\n",
            "ndcg:0.3468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 79/79 [00:01<00:00, 60.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 6, total_loss: 0.3978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "eval: 100%|██████████| 943/943 [00:02<00:00, 361.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall:0.1893\n",
            "precision:0.0758\n",
            "ndcg:0.3468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 79/79 [00:01<00:00, 60.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 7, total_loss: 0.3797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "eval: 100%|██████████| 943/943 [00:02<00:00, 365.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall:0.1884\n",
            "precision:0.0764\n",
            "ndcg:0.3465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 79/79 [00:01<00:00, 56.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 8, total_loss: 0.3688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "eval: 100%|██████████| 943/943 [00:03<00:00, 290.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall:0.1894\n",
            "precision:0.0770\n",
            "ndcg:0.3475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 79/79 [00:01<00:00, 58.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 9, total_loss: 0.3624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "eval: 100%|██████████| 943/943 [00:02<00:00, 366.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall:0.1892\n",
            "precision:0.0778\n",
            "ndcg:0.3491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 79/79 [00:01<00:00, 58.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, total_loss: 0.3581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "eval: 100%|██████████| 943/943 [00:02<00:00, 365.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall:0.1894\n",
            "precision:0.0784\n",
            "ndcg:0.3496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test: 100%|██████████| 943/943 [00:03<00:00, 247.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall:0.1795\n",
            "precision:0.0753\n",
            "ndcg:0.3454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, n_epoch+1):\n",
        "    model.train()\n",
        "    total_loss= []\n",
        "    for i, batch_data in enumerate(tqdm(train_loader, desc=\"train\")):\n",
        "        users, pos_items, neg_items = batch_data\n",
        "        layered_emb_user, layered_pos_emb, layered_neg_emb = model(users, pos_items, neg_items)\n",
        "        loss = bpr_loss(layered_emb_user, layered_pos_emb, layered_neg_emb)\n",
        "        total_loss.append(loss.item())\n",
        "\n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"epoch: {epoch}, total_loss: {np.mean(total_loss):.4f}\")\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    recall = np.array([])\n",
        "    precision = np.array([])\n",
        "    ndcg = np.array([])\n",
        "    for u, true in tqdm(val_true.items(), desc=\"eval\"):\n",
        "        # 유저별 소비하지 않은 아이템\n",
        "        unconsumed_items = list(set(all_items)-set(user_consumed[u]))\n",
        "        unconsumed_items = torch.tensor(unconsumed_items).to(device)\n",
        "        u = torch.tensor(u).to(device)\n",
        "\n",
        "        # 추론\n",
        "        layered_emb_user, layered_emb_item, _ = model(u, unconsumed_items)\n",
        "        pred = (layered_emb_user * layered_emb_item).sum(dim=-1)\n",
        "        _, pred_idx = torch.topk(pred, k=top_k)\n",
        "        top_k_items = unconsumed_items[pred_idx].tolist()\n",
        "\n",
        "        # 메트릭\n",
        "        recall=np.append(recall, get_recall(top_k_items, true, k=top_k))\n",
        "        precision=np.append(precision, get_precision(top_k_items, true, k=top_k))\n",
        "        ndcg=np.append(ndcg, get_nDCG(top_k_items, true, k=top_k))\n",
        "    print(f\"recall:{np.mean(recall):.4f}\\nprecision:{np.mean(precision):.4f}\\nndcg:{np.mean(ndcg):.4f}\")\n",
        "\n",
        "# validation\n",
        "model.eval()\n",
        "recall = np.array([])\n",
        "precision = np.array([])\n",
        "ndcg = np.array([])\n",
        "for u, true in tqdm(test_true.items(), desc=\"test\"):\n",
        "    # 유저별 소비하지 않은 아이템\n",
        "    unconsumed_items = list(set(all_items)-set(user_consumed[u]))\n",
        "    unconsumed_items = torch.tensor(unconsumed_items).to(device)\n",
        "    u = torch.tensor(u).to(device)\n",
        "\n",
        "    # 추론\n",
        "    layered_emb_user, layered_emb_item, _ = model(u, unconsumed_items)\n",
        "    pred = (layered_emb_user * layered_emb_item).sum(dim=-1)\n",
        "    _, pred_idx = torch.topk(pred, k=top_k)\n",
        "    top_k_items = unconsumed_items[pred_idx].tolist()\n",
        "\n",
        "    # 메트릭\n",
        "    recall=np.append(recall, get_recall(top_k_items, true, k=top_k))\n",
        "    precision=np.append(precision, get_precision(top_k_items, true, k=top_k))\n",
        "    ndcg=np.append(ndcg, get_nDCG(top_k_items, true, k=top_k))\n",
        "print(f\"recall:{np.mean(recall):.4f}\\nprecision:{np.mean(precision):.4f}\\nndcg:{np.mean(ndcg):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}